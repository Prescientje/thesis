\documentclass[]{report}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,wrapfig}


% Title Page
\begin{titlepage}
\centering
\title{Identifying Hand Hygiene Using Neural Networks}
\author{James H. Edwards \\ advised by Dr. Valerie Galluzzi, Dr. Matthew Boutell, and Dr. Klaus Baer\\
	\\
	\\
	A thesis submitted in partial fulfillment
	of the requirements for the \\
	Bachelor of Science degrees in International Computer Science \\
	 at the Rose-Hulman Institute of Technology and Hochschule Ulm.}
	
\end{titlepage}
\begin{document}
\maketitle
\tableofcontents



\begin{abstract}
	Both machine and deep learning are growing fields of computer science that are rapidly increasing in relevance to our society. One compelling field of application is in healthcare and hospitals. Systems can be designed to help and improve the lives of patients with particular diseases or disabilities, and systems can even be trained to diagnose complicated symptoms or to otherwise aid doctors in their duties. The experiment used in this project was originally conducted by Dr. Valerie Galluzzi, who used custom 3-D wrist accelerometer sensors in order to measure healthcare worker compliance to hospital guidelines. My continuation of the experiment took the data and used neural networks to generate models that can predict when a novel sample is performing hand hygiene. 
\end{abstract}

\chapter{Background}

This part of the thesis provides an introduction to the concepts of deep learning and neural networks, followed by the Literature Review I conducted to gather information about other recent work in this field.

\section{Hand Hygiene}


--github

--link to thesis pdf, poster, and final slides

--keep code + such on repo


One of the most effective techniques to prevent the spread of infection in hospitals is having healthcare workers (such as doctors and nurses) follow proper hand hygiene guidelines \cite{Galluzzi}. To help achieve this task, many organizations, including the World Health Organization, have created standards and guides for how to properly perform hand hygiene. They have set out the "5 Moments of Hand Hygiene" \cite{WHO}:
\begin{enumerate}
	\item Before touching a patient
	\item Before clean/aseptic procedure
	\item After body fluid exposure risk
	\item After touching a patient
	\item After touching patient surroundings
\end{enumerate}

\section{Deep Learning}
\begin{enumerate}
	\item Simulate the brain
	\item Convolution
	\item Testing vs training data
\end{enumerate}


Yann LeCun, Yoshua Bengio, and Geoffrey Hinton wrote about the big picture and history of deep learning. They state the key differentiation between machine and deep learning is “these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure” \cite{ThreeGiants}. 

They also discuss the main points of supervised learning: one uses a large dataset with labels that indicate the category or value of each input data point, then the learning algorithm attempts to use a function with many many parameters to match the correct output, and the algorithm is able to change the parameters (based on what is called backpropagation using a gradient vector based on the loss function) to better fit the model and then the process is repeated until an accuracy threshold or step count has been reached. Being a better fit refers to the loss function, which measures the differences between the array of actual results and the array of the predicted results for each data point. Specific examples of loss functions are discussed later.

The authors also go into detail about the finer points of stochastic gradient descent (SGD) and talk about how using small sets of examples is better than going one example at a time (essentially because things averaging out is better). The authors also explain the Rectified Linear Unit (ReLU) and other output functions, which introduce non-linearity in the matrix multiplications so the affine system of equations does not collapse. The Leaky ReLU has a slightly negative slope instead of any $x < 0$ having the value of $0$.

These authors also discussed CNNs, normally used for image data, and the different types of layers they use: convolutional layers, pooling layers, and fully-connected layers. Convolutional layers use patches of weights, usually much smaller than an image but also “deeper” in the 3rd dimension, which then help identify different parts of an image. CNNs have been hugely successful in the field of image recognition, which also aids in the effectiveness of self-driving cars. CNNs are now even able to caption images, showing that they really “understand” the picture and do not just see pixels of various colors.
The author also introduced the idea of recurrent neural networks (RNNs). These are better suited for problems which “involve sequential inputs, such as speech and language.” They take in each input one at a time in order, going through the algorithm through each input while also accounting for the newer inputs, before outputting a value. These networks have trouble with storing data for a long time, so they are often combined with Long Short-Term Memory Cells (LSTMs), which is a unit that has hidden layers which allow for storing values and remembering them or clearing them based on inputs.

The authors also claim that unsupervised learning will become very important, and also believe that “natural language understanding” will be a major field of impact for deep learning. However, they note that we will need completely new systems and methods to achieve “major progress in artificial intelligence” \cite{ThreeGiants}.


The website “Neural Networks and Deep Learning,” written by Michael Nielsen, provides a excellent and quite detailed introduction to deep learning by explaining the Mixed National Instrument of Standards and Technology (MNIST) dataset and networks that have high accuracy in classifying the dataset. He mentions that perceptrons multiply their inputs by a particular weight and then a bias is added, and then that value is output (perhaps into the sigmoid or ReLU function). Technically a cost function, usually Mean Squared Error (MSE) or Cross Entropy (CE) is used to find the error between the projected outputs and the real outputs, and then the weights and biases are adjusted through backpropagation. He also goes into great detail about backpropagation and explains how putting everything into vectors and then using a graph of calculations can make the calculations simpler and make it possible to compute all of the partial derivatives in one pass. He explains some types of regularization, namely L2 (attempting to limit the total value of the weight matrix), dropout (randomly removing nodes in a network), and early stopping, all of which are designed to avoid overfitting / overtraining. He mentions that one can add more data to a training set by slightly changing his dataset (for example, adding in tilted images), which helps the training process. Nielsen also mentions the vanishing gradient problem, which is the idea that a small change in any layer can cascade throughout the rest of the layers and slow down learning. He then talks about CNNs in depth \cite{NNDL}.

\section{Literature Review}

\subsection{Video-Based Systems}

Neverova, Wolf, Taylor, and Nebout claim that gesture identification has several challenges: “cultural and individual differences in tempos and styles of articulation, variable observation conditions, …, [and] infinitely many kinds of out-of-vocabulary motion”, among others \cite{Neverova}. Their network design, a CNN which took in intensity and depth video, along with “articulated pose information extracted from depth maps” won the 2014 ChaLearn Challenge on Multi-modal Gesture Recognition. The general system used a skeletal mapping program to try to identify the various parts of the body from a video/image based on different frame stride lengths. Their main design improvement was to use “ModDrop” to fuse or not fuse certain channels together and to choose when to fuse them in the pipeline process. 

Starner, Weaver, and Pentland worked on a system to recognize ASL in real time. They designed two systems which used HMMs and “tracked unadorned hands” to classify signs based on a 40 word lexicon. One system was a camera on a desk looking at a signer, and the other was a camera on a hat worn by a signer (trying to identify his own signs). By using a “strong part-of-speech grammar” they achieved a test accuracy of over 87\% for the first system and over 97\% for the second. The authors did express some concern over the potential issues with increasing the possible word-count as well as the variance between different signers and mentioned that perhaps gloves or finger sensors may be needed, as well as gathering much more data overall \cite{Starner98}.

Shin and Sung claim that gesture recognition is of vital importance for wearables. These researchers developed “dynamic hand gesture recognition techniques,” one of which used a CNN and an RNN combined taking in video data, while the other only used an RNN but used accelerometer data \cite{ShinS16}. The RNN that used accelerometer data uses LSTMs with the standard 3 gates (input, forget, output). However, trying to compress the floating points came at significant accuracy cost. With only two bits of quantization, the error rate was 32.77\%. Three bits gave an error rate of 28.69\% but 4 bits gave 11.43\%. Doing so reduced the memory requirements by over 90\%. 

\subsection{Acceleration-Based Systems}

Hammerla, Halloran, and Plötz state that the main technique in HAR “includes sliding window segmentation of time-series data captured with body-worn sensors, manually designed feature extraction procedures, and a wide variety of (supervised) classification methods” \cite{Hammerla}. Their paper covers LSTM models, a CNN, and a regular deep network applied to the HAR problem, specifically covering 3 problems: “manipulative gestures, repetitive physical activities, and a medical application … Parkinson’s disease”. Their deep network had up to five hidden layers and used either dropout or max-in norm for regularization with mini-batches of size 64. This network achieved just over 90\% accuracy on the RPA dataset but got almost 60\% on the other two datasets. The other networks did much better over the entire problem set.

Bulling, Blank, and Schiele set out to make a comprehensive overview of the HAR problem with “body-worn inertial sensors”. They first mention several fields that would benefit from activity recognition: the industrial, sports, entertainment, and healthcare sectors \cite{Bulling}. They noted several applications and devices such as the Wii and Kinects as well as the Nike+ shoes which help track activity. They mentioned several key challenges that the field of HAR has: no clear definition of specific activities, the various possible composition of sensors, and the specific evaluation metrics for each application. Other challenges include intraclass variability, interclass similarity, the Null class problem, the diversity of physical activities, class imbalance, the annotation of ground truth, data collection and experiment design, variability in sensors, and system design. I can certainly understand how these issues can cause major problems in identification. For my data I am not exactly sure how much effect intraclass variability has in my classification system, however people certainly wash their hands in different ways, perhaps by doing motions in different orders. Being left- or right-handed also could play a role. Of huge importance to me is dealing with class imbalance, because my dataset is over 95\% “not-hand-hygiene” samples. The authors also propose a system model called the Activity Recognition Chain which has the following steps: data acquisition, signal preprocessing, segmentation, feature extraction and selection, training, and classification.

Lester, Choudhury, and Borriello designed a “personal activity recognition system” to be used by anyone. They used a single sensor that could be put in different locations on the body but then had multiple types of sensors. Twelve subjects performed 8 activities over a few days “carrying a collection of sensors worn in three different locations on the body.” The researchers wanted to find out if the location of the sensor mattered, how much values changed between users, and how many sensors are actually required to “recognize a significant set of basic activities?” \cite{Lester2006}.
Using HMMs they got between 80\% and 90\% accuracy for each sensor location, as well as all sensors combined. They found that the location of the sensor did not really matter, the system worked well on a new subject, and only 3 types of sensors were needed to do the job well: audio, barometric pressure, and accelerometer.


\chapter{Experiment}

Now that the reader has an introduction to the neural networks and a view on the current research in deep learning in the healthcare field, I will describe the experiment I undertook. It begins with a discussion of the data and some issues I encountered, the models I developed to train, and other deep learning techniques I used to increase the accuracy of my models.

\section{Initial Steps}

My project is a continuation of the work of Dr. Valerie Galluzzi, who did her dissertation on using machine learning techniques to identify if various healthcare workers were being compliant with the guidelines of the World Health Organization. She and her team** worked to develop custom wrist-wearable acceleration sensors, which sent data to a separate device. The data could be offloaded for investigation. I was given most of the data she used, which consisted of X total samples from Y healthcare workers in Z hospital in A year. These acceleration values were taken at 100 Hz, and measured the X, Y, and Z values for each hand for varying lengths of time (the data for each hand was recorded on separate devices but the values were stored together). 

\begin{quote}
Put in the data columns for the IPython notebook CSV part
\end{quote}

I initially worked on reorganizing the data. It had been given to me in a JSON file, whereas it is much easier to work with CSV files that can be imported in Pandas, a Python module that can be combined with TensorFlow (Google's Deep Learning API). Once I had a Python script which converted the data to CSVs, I then made a second Python script to load the data from the CSV files and put the various acceleration data points into separate X, Y, and Z matrices. These matrices served as the inputs to the Tensorflow models.

The initial layout of the data is shown below, to the left of the data layout I used for the majority of the project:\\
\begin{minipage}{\linewidth}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/accuracy}
	\end{minipage}
	\hfill
	
	\begin{minipage}{0.49\textwidth}
		\centering
		Sample image.
	\end{minipage}
\end{minipage}

DATA SLICING DISCUSSION\\
Because of the way the data was collected, I did not actually have a continuous stream of data; that is, each sample did not exactly or directly take place immediately before or after another sample. Therefore I had to work with the individual slices provided to me, rather than a "constant" stream of values I could divide up any way I wished.

DATA IMBALANCE DISCUSSION\\
One important aspect of the data to be discussed is that over 95\% of the total length of all data samples were from Non-Hand-Hygiene samples. This imbalance led to various discussions about how it should be resolved, as my initial tests simply ignored the HH samples around 96\% accuracy. 

FINAL PIPELINE SUMMARY ??

\section{Models}

I began with 4 models, here listed with the names I gave them:
\begin{enumerate}
	\item Original Model: A simple model with no hidden layer, which took in the $X,Y,Z$ values.
	\item Complex Model: A model with no hidden layer but used the $X,Y,Z,X^{2},X^{2},X^{2},X*Y,Y*Z,Z*X$ values as inputs.
	\item Layered Model: A model with 1 hidden layer but only the $X,Y,Z$ values as inputs.
	\item XYZ Model: A model with a hidden layer but the data was arranged with the concept of the \textit{previous, now,} and \textit{next} instances.\\
\end{enumerate}

Following a standard convention for neural network models, I will illustrate the models below.
In clockwise order beginning with top left, these are the visualizations for the Original model, the Complex model, the XYZ model, and finally the Layered model.

\begin{minipage}{\linewidth}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/original}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/complex}
	\end{minipage}
\end{minipage}
\begin{minipage}{\linewidth}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/layered}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/xyz1}
	\end{minipage}
\end{minipage}
\\

I then added a Convolutional Model, and I also added more layers to the Complex model and the Layered model in an attempt to improve accuracy.

The convolutional model ...

The XYZ model ...


$$
\left[
\begin{array}{cccccccccccccc}
x_{1} & y_{1} & z_{1} & x_{2} & ... & x_{n+1} & y_{n+1} & z_{n+1} & x_{n+2} & ... & x_{2*n+1} & y_{2*n+1} & ... & z_{3*n} \\
x_{n+1} & y_{n+1} & z_{n+1} & x_{n+2} & ... & x_{2*n+1} & y_{2*n+1} & z_{2*n+1} & x_{2*n+2} & ... & x_{3*n+1} & y_{3*n+1} & ... & z_{4*n} \\
\vdots & & (previous) & & \vdots & & (now) & &  & \vdots & & (next) & \ddots\\
\end{array}
\right]
$$

As mentioned before, I also simply added four hidden layers to the Layered Model and the Complex Model. I will not show the illustration here, but these hidden layers consisted of 512 nodes \cite{Goodfellow-et-al-2016}. 

\section{Other Techniques}

With the models which had multiple layers, I also implemented L2 regularization. This technique attempts to prevent overfitting by reducing the total value of the weight matrices. Thus the model cannot become overtrained on the training data and score lower on the testing data \cite{Hammerla}.

Convolutional Model Attempt

XYZ Model Attempt

\chapter{Results}

This part covers the results of the experiments I ran, described in the previous part of this thesis.

\section{Recall}

For all the tests I ran, I recorded statistics gathered from the confusion matrix. One of the main values one can gather from a confusion matrix is recall, which is $\frac{truepositives}{truepositives + falsenegatives} $ \cite{ThreeGiants}.


\begin{minipage}{\linewidth}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/recall}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		Sample image.
	\end{minipage}
\end{minipage}

\section{Accuracy}

\begin{minipage}{\linewidth}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/accuracy}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		Sample image.
	\end{minipage}
\end{minipage}

\section{Other Views}

\begin{minipage}{\linewidth}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/accuracy-deep}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{../images/handed-original}
	\end{minipage}
\end{minipage}

\chapter{Discussion}

Now that the reader is familiar with my experiment and the results of it, I will enter into some discussion about the impact of what I have done as well as future work that could be done in this vein.

\section{Impact}

The main motivation for this work was to find a better way to ensure that healthcare workers are adequately washing their hands to prevent the spread of diseases in hospitals. While my best result of 85\% is by no means a perfect 100\%, I feel that this system would be an important first step to increasing the amount of times a healthcare worker would wash his or her hands.

One important factor is using the wrist-sensor system compared to any other would be the cost. These sensors would not be too expensive to produce, and would definitely be cheaper (and perhaps more accurate) than installing special sensors near every sink or hand sanitizer dispenser and then constructing a system to measure the amount of time a doctor or nurse is within a certain distance of a hand-washing location.

\section{Future Work}

If one were interested in further developing the physical system, it could be interesting to investigate using different sensors, such as velocity, relative location, and/or angular acceleration, and determining if a particular combination is more accurate at measuring hand hygiene.

Of course, one could also try to develop a more complicated neural network model or implement future deep learning techniques in order to improve the recall and accuracy of the system.

Another important area to look into would be utilizing a video input of hand movements, perhaps with a depth camera or just an RGB camera. The video input could also be combined with acceleration data for increased effectiveness \cite{NNDL}.

Something else to look into would identifying proper hand hygiene technique, compared to simply detecting "hand hygiene or not" for a particular sample. Measuring the effective of the hand hygiene may need many more sensors, as discussed above. It could also be difficult to express proper technique in a way that would generalize for many subjects \cite{Neverova}.




$$
\left[
\begin{array}{cccc}
x_{1} & x_{2} & ... & x_{n} \\
x_{n+1} & x_{n+2} & ... & x_{2*n} \\
 \vdots & & \ddots & \\


\end{array}
\right]
$$




\bibliographystyle{plain}
\bibliography{./bibfile}
\end{document}          
