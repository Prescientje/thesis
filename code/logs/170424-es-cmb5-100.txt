complex-modelbuilder5 early-stop TEST
This does not save any model files.
Sample length = 100
Learning rate = 0.001000
Stddev = 0.020000
Alpha = 0.002000
Superfactor = 13
Epochs = 100
Batch size = 8000
lmodelname = savedModels-complex/100-left/100-left
 
starting tensorflow section
 
 
Trial 1 savedModels-complex/100-left/100-left
 
00 -- 0.7998749820 -- 0.8058512940 -- 12.2220191956 -- 12.2633476257
01 -- 0.8129826417 -- 0.8023889899 -- 8.0042867661 -- 7.9933276176
02 -- 0.8101553109 -- 0.7095126807 -- 7.2133164406 -- 7.5394229889
03 -- 0.8602586912 -- 0.7444819527 -- 6.8821878433 -- 7.1767568588
04 -- 0.8857527528 -- 0.7930407686 -- 6.6467394829 -- 6.8513503075
05 -- 0.8903495696 -- 0.8211719900 -- 6.4566555023 -- 6.6168284416
06 -- 0.8992450834 -- 0.7931273262 -- 6.2457647324 -- 6.4746270180
07 -- 0.9060537578 -- 0.8181424738 -- 6.0522141457 -- 6.2480211258
08 -- 0.9076020580 -- 0.7998788194 -- 5.8556294441 -- 6.0958871841
09 -- 0.9145453671 -- 0.8018696442 -- 5.6645836830 -- 5.9070825577
10 -- 0.9185651777 -- 0.8184021466 -- 5.4795150757 -- 5.7064962387
11 -- 0.9110063952 -- 0.7781528607 -- 5.3076782227 -- 5.6192607880
12 -- 0.9234312641 -- 0.8019562019 -- 5.1227121353 -- 5.4018387794
13 -- 0.9282204164 -- 0.8061109668 -- 4.9510083199 -- 5.2282814980
14 -- 0.9223541857 -- 0.7786722064 -- 4.7977209091 -- 5.1342806816
15 -- 0.9074674232 -- 0.7433567039 -- 4.6611642838 -- 5.0887603760
16 -- 0.8903014858 -- 0.8342421882 -- 4.5669188499 -- 4.7741184235
17 -- 0.9117180363 -- 0.8327707089 -- 4.3775324821 -- 4.5934567451
18 -- 0.9271241044 -- 0.8294815200 -- 4.2048220634 -- 4.4557466507
19 -- 0.9350098572 -- 0.8254998702 -- 4.0555820465 -- 4.3332829475
20 -- 0.9388854162 -- 0.8235956029 -- 3.9187602997 -- 4.2209539413
21 -- 0.9286916382 -- 0.7662079114 -- 3.8076455593 -- 4.2331938744
22 -- 0.9418281483 -- 0.8203929715 -- 3.6647801399 -- 3.9830431938
23 -- 0.9462230129 -- 0.7930407686 -- 3.5443160534 -- 3.9444174767
24 -- 0.9482810021 -- 0.8092270406 -- 3.4280548096 -- 3.7926952839
25 -- 0.9495792662 -- 0.8074093309 -- 3.3150880337 -- 3.7076439857
26 -- 0.9256815887 -- 0.8287025015 -- 3.2519295216 -- 3.5632710457
27 -- 0.9329999519 -- 0.7942525751 -- 3.1432561874 -- 3.5291054249
28 -- 0.9315382026 -- 0.8295680776 -- 3.0491571426 -- 3.3532524109
29 -- 0.9434822330 -- 0.8181424738 -- 2.9386537075 -- 3.2971012592
30 -- 0.9485695052 -- 0.8177962434 -- 2.8376526833 -- 3.2178621292
31 -- 0.9544357359 -- 0.8046394876 -- 2.7424168587 -- 3.1838424206
32 -- 0.9518295908 -- 0.8210854324 -- 2.6585450172 -- 3.0537064075
33 -- 0.9491657451 -- 0.7697567731 -- 2.5905165672 -- 3.1131286621
34 -- 0.9429629273 -- 0.8290487319 -- 2.5185110569 -- 2.8881983757
35 -- 0.9553493292 -- 0.8202198563 -- 2.4202036858 -- 2.8367300034
36 -- 0.9572823003 -- 0.8120834415 -- 2.3434281349 -- 2.8029394150
37 -- 0.9267875174 -- 0.7260451831 -- 2.3324942589 -- 3.0104734898
38 -- 0.9574746358 -- 0.7785856487 -- 2.2190756798 -- 2.8151400089
39 -- 0.9425974900 -- 0.8226434692 -- 2.1673152447 -- 2.6036591530
40 -- 0.9393374044 -- 0.8253267550 -- 2.1274926662 -- 2.5259418488
41 -- 0.9556955330 -- 0.8060244092 -- 2.0301125050 -- 2.5479388237
42 -- 0.9603981343 -- 0.8040335844 -- 1.9606554508 -- 2.4993429184
43 -- 0.9624945906 -- 0.8083614646 -- 1.8989269733 -- 2.4416723251
44 -- 0.9625330577 -- 0.7918289622 -- 1.8451112509 -- 2.4410898685
45 -- 0.9649468673 -- 0.7993594737 -- 1.7865122557 -- 2.3703432083
46 -- 0.9663124489 -- 0.7999653770 -- 1.7318303585 -- 2.3472332954
47 -- 0.9586094148 -- 0.8018696442 -- 1.7003104687 -- 2.2688794136
48 -- 0.9620810694 -- 0.7889725612 -- 1.6496503353 -- 2.2947542667
49 -- 0.9637640044 -- 0.7870682939 -- 1.5985941887 -- 2.2683842182
50 -- 0.9662643651 -- 0.7833463170 -- 1.5502557755 -- 2.2654166222
51 -- 0.9643890946 -- 0.8035142387 -- 1.5079132318 -- 2.1051385403
52 -- 0.9570418810 -- 0.7604085519 -- 1.4827375412 -- 2.3044645786
53 -- 0.9627061595 -- 0.7838656626 -- 1.4362661839 -- 2.1115236282
54 -- 0.9440015387 -- 0.7496754090 -- 1.4305768013 -- 2.2319364548
55 -- 0.9637736212 -- 0.7816151649 -- 1.3591510057 -- 2.0869865417
56 -- 0.9682358032 -- 0.8055916212 -- 1.3084369898 -- 1.9839154482
57 -- 0.9682358032 -- 0.7842118930 -- 1.2748086452 -- 2.0598003864
58 -- 0.9714670385 -- 0.8024755475 -- 1.2312600613 -- 1.9678869247
59 -- 0.9502235899 -- 0.7449147408 -- 1.2403122187 -- 2.2789480686
60 -- 0.9585709477 -- 0.8140742664 -- 1.1888037920 -- 1.8425916433
61 -- 0.9694378997 -- 0.7913961742 -- 1.1416474581 -- 1.9656581879
62 -- 0.9140356782 -- 0.8186618194 -- 1.2323977947 -- 1.8707346916
63 -- 0.9474924268 -- 0.8064571973 -- 1.1335263252 -- 1.7320189476
64 -- 0.7888060778 -- 0.6377564269 -- 1.5073666573 -- 2.5375721455
65 -- 0.9105063230 -- 0.8142473816 -- 1.2020407915 -- 1.6382765770
66 -- 0.9413953936 -- 0.7888860036 -- 1.0839397907 -- 1.6590511799
67 -- 0.9503966918 -- 0.8035142387 -- 1.0404213667 -- 1.6704154015
68 -- 0.9601288647 -- 0.7793646672 -- 0.9997937679 -- 1.7188472748
69 -- 0.9609559071 -- 0.8012637410 -- 0.9684416652 -- 1.6248055696
70 -- 0.9653988556 -- 0.7816151649 -- 0.9363316298 -- 1.7227300406
71 -- 0.9674376112 -- 0.7848177962 -- 0.9065391421 -- 1.6935625076
72 -- 0.9687358754 -- 0.7889725612 -- 0.8831167221 -- 1.7058415413
73 -- 0.9555031976 -- 0.7517527915 -- 0.8851021528 -- 1.8342828751
74 -- 0.9597057268 -- 0.8106985199 -- 0.8525341749 -- 1.5642975569
75 -- 0.9696302351 -- 0.7931273262 -- 0.8150272369 -- 1.6362977028
76 -- 0.9618214166 -- 0.8069765429 -- 0.8092545867 -- 1.5566724539
77 -- 0.9632639323 -- 0.8074093309 -- 0.7899085283 -- 1.5047256947
78 -- 0.9690339953 -- 0.7833463170 -- 0.7599439621 -- 1.6789983511
79 -- 0.9540414483 -- 0.7508872154 -- 0.7724270821 -- 1.8635115623
80 -- 0.9690339953 -- 0.7956374968 -- 0.7250062823 -- 1.5751032829
81 -- 0.9692936481 -- 0.8004847226 -- 0.7031345963 -- 1.5693330765
82 -- 0.9573784680 -- 0.8121699991 -- 0.7141500711 -- 1.4906716347
83 -- 0.9688224263 -- 0.8017830866 -- 0.6721719503 -- 1.5399391651
84 -- 0.9746405732 -- 0.7842118930 -- 0.6471065283 -- 1.6167433262
85 -- 0.9729864884 -- 0.7777200727 -- 0.6332213283 -- 1.6761977673
86 -- 0.9739385488 -- 0.7888860036 -- 0.6151112318 -- 1.5646165609
87 -- 0.9677261143 -- 0.7992729161 -- 0.6124269366 -- 1.5394326448
88 -- 0.9758330528 -- 0.7907902709 -- 0.5839436054 -- 1.6048864126
89 -- 0.4508631053 -- 0.3837098589 -- 29.1936149597 -- 34.7897796631
90 -- 0.7277876617 -- 0.6354193716 -- 5.5564231873 -- 7.6663570404
91 -- 0.8375823436 -- 0.7714879252 -- 1.5441833735 -- 1.8546364307
92 -- 0.7629850459 -- 0.6635505929 -- 1.7600624561 -- 2.0633125305
93 -- 0.8551521854 -- 0.8023024323 -- 1.0032194853 -- 1.0812675953
94 -- 0.8628744530 -- 0.8104388471 -- 0.9085384607 -- 0.9908229113
95 -- 0.8672500841 -- 0.8002250498 -- 0.8850235343 -- 0.9801795483
96 -- 0.8677501563 -- 0.7822210681 -- 0.8698088527 -- 0.9945826530
97 -- 0.8729239794 -- 0.7951181511 -- 0.8521925211 -- 0.9641411304
98 -- 0.8739914411 -- 0.7936466719 -- 0.8380017281 -- 0.9527310729
99 -- 0.8747511660 -- 0.8116506535 -- 0.8272197247 -- 0.9230616093
 
btrain accuracy score = 0.874751
btest  accuracy score = 0.811651
btrain confusion matrix =
[[38302  8570]
 [ 4454 52659]]
btest  confusion matrix =
[[3435  996]
 [1180 5942]]
 
 
Both Train Confusion Vals
Miss Rate   = 0.182838
Fallout     = 0.077986
Precision   = 0.895827
Recall      = 0.817162
Accuracy    = 0.874751
Specificity = 0.922014
Both Test Confusion Vals
Miss Rate   = 0.224780
Fallout     = 0.165684
Precision   = 0.744312
Recall      = 0.775220
Accuracy    = 0.811651
Specificity = 0.834316
Both Test Confusion Matrix
[[ 3435.   996.]
 [ 1180.  5942.]]
 
average train score   = 
[ 0.          0.          0.08747512]
average test score    = 
[ 0.          0.          0.08116507]
best test score       = 
[ 0.          0.          0.81165065]
worst test score      = 
[ 100.          100.            0.81165065]
